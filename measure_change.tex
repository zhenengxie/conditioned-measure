\documentclass[draft]{scrartcl}

\usepackage{mathtools, amsfonts, dsfont}
\usepackage{diffcoeff}

\usepackage[backend=biber]{biblatex}

\addbibresource{references.bib}

\usepackage{varioref}
\usepackage{hyperref}
\usepackage{cleveref}

\usepackage{thmtools}

\declaretheorem[thmbox=M]{theorem}
\declaretheorem[thmbox=M]{lemma}

\usepackage{amsthm}

\newcommand{\R}{\mathbb R}
\newcommand{\N}{\mathbb N}
\newcommand{\E}{\mathbb E}
\newcommand{\Z}{\mathbb Z}
\newcommand{\littleo}{o}
\newcommand{\bigo}{O}
\newcommand{\biasD}{\hat{D}}
\newcommand{\unconmc}{\psi}
\newcommand{\conmc}{\phi}
\newcommand{\defeq}{:=}
\newcommand{\eqdist}{\overset{d}{=}}

\newcommand{\indi}{\mathds{1}}

\newcommand{\vect}{\mathbf}
\renewcommand{\Pr}{\mathbb P}
\newcommand{\Qr}{\mathbb Q}

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}

\newcommand*\dif{\mathop{}\!\mathrm{d}}
\newcommand{\biasvar}{\sigma_-^2}
\newcommand{\diffsd}{\tilde{\sigma}}
\newcommand{\omegaevent}{\mathcal E}

\title{Measure change under conditioning}
\author{}
\date{}

\begin{document}

\maketitle

\section{Setup and notation}

\begin{itemize}
    \item Let $\vect{D} = (D^-, D^+)$ a $\N \times \N$-valued random variable and let 
        \begin{align}
            \mu &\defeq \E[D^-] = \E[D^+] \\
            \beta_- &\defeq \E[(D^-)^2] \\
            \gamma_- &\defeq \E[(D^-)^3] \\
            \rho &\defeq \E[D^- D^+]
        \end{align}
    \item Let $\vect{D}_1, \ldots, \vect{D}_n$ be i.i.d.\ copies of $\vect{D}$ let 
        \begin{align}
            \Xi^{\pm}_{n-m} &\defeq \sum_{i = m + 1}^n D^{\pm}_i \\
            \Delta_{n-m} &\defeq \Xi^-_{n-m} - \Xi^+_{n-m}.
        \end{align}
    \item Let $\Sigma_n$ be a sized biased ordering of $\vect{D}_1, \ldots, \vect{D}_n$ be in-degree. Rigirously $\Sigma_n$ is a random permutation on $\{1, \ldots, n\}$ such that
        \begin{equation}
            \Pr(\Sigma_n = \sigma \mid D^-_1 = k^-_1, \ldots, D^-_n = k^-_n) \defeq
            \prod_{i=1}^n \frac{k^-_{\sigma(i)}}{\sum_{j=i}^n k^-_{\sigma(j)}}.
            %\frac{k^-_{\sigma(1)}}{\sum_{i=1}^n k^-_{\sigma(i)}} \frac{k^-_{\sigma(2)}}{\sum_{i=2}^n k^-_{\sigma(i)}} \cdots
            %\frac{k^-_{\sigma(n)}}{k^-_{\sigma(n)}}.
        \end{equation}
    \item Let $(\vect{\biasD}_{n, 1}, \ldots, \vect{\biasD}_{n, n}) \defeq (\vect{D}_{\Sigma_n(1)}, \ldots, \vect{D}_{\Sigma_n(n)})$.
    \item Let $\vect{Z}_1, \vect{Z}_2, \ldots$ be i.i.d.\ with law
        \begin{equation}
            \Pr(Z^-_i = k^-, Z^+_i = k^+) \defeq \frac{1}{\mu} k^- \Pr(D^- = k^-, D^+ = k^+).
        \end{equation}
        Then
        \begin{align}
            \nu_- &\defeq \E[Z^-_i] = \tfrac{\beta_-}{\mu} \\
            \nu_+ &\defeq \E[Z^+_i] = \tfrac{\rho}{\mu}
        \end{align}
\end{itemize}

\section{Exact form of the measure change}

First we state the form of the measure change between $\vect{\biasD}_{n, 1}, \ldots, \vect{\biasD}_{n, m}$ and $\vect{Z}_1, \ldots, \vect{Z}_m$. The proof of \cite[Proposition 4.2]{conchon--kerjan_stable_2020} covers the undirected configuration model but the same proof works for the directed case.

\begin{theorem}
    \label{thm:unconditioned-mc}
    For any test function $u: (\N \times \N)^m \to \R$, 
    \begin{equation}
        \E[u(\vect{\biasD}_{n, 1}, \ldots, \vect{\biasD}_{n, m})] =
        \E[u(\vect{Z}_1, \ldots, \vect{Z}_m) \unconmc^n_m(\vect{Z}_1, \ldots, \vect{Z}_m)]
    \end{equation}
    where
    \begin{equation}
        \unconmc^n_m(\vect{k}_1, \ldots, \vect{k}_m) \defeq
        \E \left[ \prod_{i=1}^m \frac{(n - i + 1)\mu}{\sum_{j=i}^n k_j^- + \Xi^-_{n - m}}\right].
    \end{equation}
\end{theorem}

Now we gives the form of the measure change after conditioning on the event
\begin{equation}
    \{\Delta_n = 0\}
    = \left\{ \sum_{i=1}^n \biasD^-_{n, i} = \sum_{i=1}^n \biasD^+_{n, i} \right\}
    = \left\{ \sum_{i=1}^n D^-_i = \sum_{i=1}^n D^+_i \right\}.
\end{equation}

\begin{theorem}
    For any test function $u: (\N \times \N)^m \to \R$, 
    \begin{equation}
        \E\left[u(\vect{\biasD}_{n, 1}, \ldots, \vect{\biasD}_{n, m}) \mid \Delta_n = 0 \right] =
        \E[u(\vect{Z}_1, \ldots, \vect{Z}_m) \conmc^n_m(\vect{Z}_1, \ldots, \vect{Z}_m)]
        \label{eq:conditioned-measure-change-condition}
    \end{equation}
    where
    \begin{equation}
        \conmc^n_m(\vect{k}_1, \ldots, \vect{k}_m) = \frac{
        \E \left[ \prod_{i=1}^m \frac{(n - i + 1)\mu}{\sum_{j=i}^n k_j^- + \Xi^-_{n - m}}
        \indi \left\{ \Delta_{n-m} = \sum_{i=1}^m (k^+_i - k^-_i) \right\}\right]
        }{\Pr(\Delta_n = 0)}.
    \end{equation}
\end{theorem}

\begin{proof}
    By \cref{thm:unconditioned-mc}, since
    \begin{equation}
        \unconmc^n_n(\vect{k}_1, \ldots, \vect{k}_m) =
        \prod_{i=1}^n \frac{(n - i + 1) \mu}{\sum_{j=i}^n k_j^-}
    \end{equation}
    we have
    \begin{align}
        &\E\left[u(\vect{\biasD}_{n, 1}, \ldots, \vect{\biasD}_{n, m}) \indi\left\{ \sum_{i=1}^n (\biasD^-_{n, i} - \biasD^+_{n, i}) = 0 \right\} \right] \\
        = &\E\left[u(\vect{Z}_1, \ldots, \vect{Z}_m) \indi\left\{ \sum_{i=1}^n (Z_i^- - Z_i^+) = 0 \right\} \prod_{i=1}^n \frac{(n - i + 1) \mu}{\sum_{j=i}^n Z_j^-} \right] \\
        = &\E\Bigg[u(\vect{Z}_1, \ldots, \vect{Z}_m) \E \Bigg[ \indi\left\{ \sum_{i=m+1}^n (Z_i^- - Z_i^+) = \sum_{i=1}^m (Z^+_i - Z^-_i) \right\} \nonumber \\
       & \hspace{5.5em} \times \prod_{i=1}^m \frac{(n - i + 1)\mu}{\sum_{j=i}^m Z_j^- + \sum_{j = m+1}^n Z_j^-} \prod_{i=m+1}^n \frac{(n - i + 1) \mu}{\sum_{j=i}^n Z_j^-} \mid \vect{Z}_1, \ldots, \vect{Z}_m \Bigg] \Bigg].
    \end{align}
    Thus \cref{eq:conditioned-measure-change-condition} holds with
    \begin{align}
        \conmc^n_m(\vect{k}_1, \ldots, \vect{k}_m)
        &= \frac{1}{\Pr(\Delta_n = 0)}\E\Bigg[ \indi\left\{ \sum_{i=m+1}^n (Z_i^- - Z_i^+) = \sum_{i=1}^m (k^+_i - k^-_i)\right\} \nonumber \\
        &\hspace{6em} \times \prod_{i=1}^m \frac{(n - i + 1)\mu}{\sum_{j=i}^m k_j^- + \sum_{j = m+1}^n Z_j^-} \prod_{i=m+1}^n \frac{(n - i + 1) \mu}{\sum_{j=i}^n Z_j^-} \Bigg].
    \end{align}
    Since the $Z_i$ are i.i.d.\ we can shift indices to get
    \begin{align}
        \conmc^n_m(\vect{k}_1, \ldots, \vect{k}_m)
        &= \frac{1}{\Pr(\Delta_n = 0)}\E\Bigg[ \indi\left\{ \sum_{i=1}^{n-m} (Z_i^- - Z_i^+) = \sum_{i=1}^m (k^+_i - k^-_i)\right\} \nonumber \\
        &\hspace{5.5em} \times \prod_{i=1}^m \frac{(n - i + 1)\mu}{\sum_{j=i}^m k_j^- + \sum_{j = 1}^{n -m} Z_j^-} \prod_{i=1}^{n-m} \frac{(n - m - i + 1) \mu}{\sum_{j=i}^{n - m} Z_j^-} \Bigg].
    \end{align}
    Note that
    \begin{equation}
        \prod_{i=1}^{n-m} \frac{(n - m - i + 1) \mu}{\sum_{j=i}^{n - m} Z_j^-} = \unconmc^{n-m}_{n-m}(\vect{Z}_1, \ldots, \vect{Z}_{n-m})
    \end{equation}
    therefore by \cref{thm:unconditioned-mc}
    \begin{align}
        \conmc^n_m(\vect{k}_1, \ldots, \vect{k}_m)
        &= \frac{1}{\Pr(\Delta_n = 0)}\E\Bigg[ \indi\left\{ \sum_{i=1}^{n-m} (\biasD_{n-m, i}^- - \biasD_{n-m, i}^+) = \sum_{i=1}^m (k^+_i - k^-_i)\right\} \nonumber \\
        &\hspace{9em} \times \prod_{i=1}^m \frac{(n - i + 1)\mu}{\sum_{j=i}^m k_j^- + \sum_{j = 1}^{n -m} \biasD_{n-m, j}^-} \Bigg].
    \end{align}
    Finally since
    \begin{equation}
        \sum_{i=1}^{n-m} \biasD^{\pm}_{n-m, i} = \sum_{i=1}^{n-m} D^{\pm}_i \eqdist \sum_{i=m+1}^n D^{\pm}_i = \Xi_{n-m}^{\pm}
    \end{equation}
    we obtain the desired result of
    \begin{equation}
        \conmc^n_m(\vect{k}_1, \ldots, \vect{k}_m) = \frac{
        \E \left[ \prod_{i=1}^m \frac{(n - i + 1)\mu}{\sum_{j=i}^n k_j^- + \Xi^-_{n - m}}
        \indi \left\{ \Delta_{n-m} = \sum_{i=1}^m (k^+_i - k^-_i) \right\}\right]
        }{\Pr(\Delta_n = 0)}.
    \end{equation}
\end{proof}

\section{Approximation of the measure change}

We explore the scaling of the measure change $\conmc^n_m$ as $n \to \infty$ in the regime $m(n) = \Theta(n^{2/3})$.

\begin{theorem}
    Define
    \begin{equation}
        s^{\pm}(i) \defeq \textstyle{\sum_{j=1}^i (k_i^{\pm} - \nu_{\pm})}.
    \end{equation}
    \begin{enumerate}
        \item Assume $\vect{k}_1, \ldots, \vect{k}_m$ satisfies
            \begin{equation}
                \label{eq:s-condition}
                \max_{i=1, \ldots, m} \abs{s^{\pm}(i)} \leq m^{\frac{1}{2} + \epsilon}.
            \end{equation}
        \item Assume $D^- - D^+$ is not supported on any proper sublattice of $\Z$, in that for all $a, b \in \Z$ if $b \geq 2$ then
            \begin{equation}
                \Pr(D^- - D^+ \in a + b\Z) < 1.
            \end{equation}
    \end{enumerate}
    Then in the regime $m = \Theta(n^{2/3})$
    \begin{equation}
        \conmc^n_m(\vect{k}_1, \ldots, \vect{k}_m)
        \geq \exp\left( \frac{1}{\mu n} \sum_{i=0}^m (s^-(i) - s^-(m)) - \frac{\biasvar}{6 \mu^2} \frac{m^3}{n^2} \right) + \littleo(1)
    \end{equation}
    where the $\littleo(1)$ term is independent of $\vect{k}_1, \ldots, \vect{k}_m$ satisfying our assumptions.
\end{theorem}

This result only provides a lower bound, but this proves to be sufficient since we can show asymptotically that the lower bound has expectation 1. This shows we haven't lost a significant amount of mass in the limit. The condition on $\vect{k}_1, \ldots, \vect{k}_m$ occurs with high probability when $\vect{k}_i = \vect{Z}_i$ since the $s^-$ and $s^+$ become centered random walks.

\subsection{Exponential tilting}

Note that $Z^- - Z^+$ is non-centered so we expect $\sum_{i=1}^m (k^+_i - k^-_i)$ to be of order $m$. This is a moderate deviation event for $\Delta_{n-m}$. To deal with this, we tilt the measure exponentially so that $\Delta_{n-m}$ has mean approximately $\sum_{i=1}^m (k^+_i - k^-_i)$. 

The next result defines the tilting and then gives asympotics for cumulant generating function of $D^-$ and for the behaviour of $\Xi^-_{n-m}$ and $\Xi^+_{n-m}$ under this tilting.

\begin{lemma}
    Define an measure $\Pr_{\theta}$, for $\theta \geq 0$, by its Radon--Nikodym derivative
    \begin{equation}
        \diff{\Pr_{\theta}}{\Pr} \defeq \exp\left( - \theta D^- - \alpha(\theta) \right)
        \quad \text{where} \quad
        \alpha(\theta) \defeq \log \E \left[ e^{-\theta D^-} \right].
    \end{equation}
    Then as $\theta \downarrow 0$ we have
    \begin{align}
        \alpha(\theta) &= -\mu \theta + \tfrac{1}{2}\var(D^-) \theta^2 - \tfrac{1}{6} \E \left[ (D^- - \mu)^3 \right] \theta^3 + \littleo(\theta^3), \\
        \E_{\theta}[D^-] &= \mu - \var(D^-) \theta + \tfrac{1}{2} \E \left[ (D^- - \mu)^3 \right] \theta^2 + \littleo(\theta^2), \\
        \E_{\theta}[D^+] &= \mu - \cov(D^-, D^+) \theta + \tfrac{1}{2} \E \left[ (D^- - \mu)^2 (D^+ - \mu) \right] \theta^2 + \littleo(\theta^2), \\
        \var_{\theta}(D^-) &= \var(D^-) - \E \left[ (D^- - \mu)^3 \right] \theta + \littleo(\theta), \\
        \var_{\theta}(D^+) &= \var(D^+) - \E \left[ (D^- - \mu)(D^+ - \mu)^2 \right] \theta + \littleo(\theta), \\
        \cov_{\theta}(D^-, D^+) &= \cov(D^-, D^+) + \E \left[ (D^- - \mu)^2 (D^+ - \mu) \right] \theta + \littleo(\theta).
    \end{align}
\end{lemma}
\begin{proof}
    Since $\E\left[\abs{D^-}^3\right] < \infty$ and $D^-$ is non-negative, by the dominated convergence theorem
    \begin{equation}
        \E \left[ (D^-)^3 \exp(-\theta D^-) \right] = \E \left[ (D^-)^3 \right] + \littleo(1)
        \label{eq:mgf-start}
    \end{equation}
    as $\theta \downarrow 0$. Integrating \cref{eq:mgf-start} with respect to $\theta$ and applying Fubini's theorem gives
    \begin{align}
        \int_0^{\theta} \E \left[ (D^-)^3 e^{-\theta' D^-}\right] \dif \theta'
        &= \int_0^{\theta} (\E \left[ (D^-)^3 \right] + \littleo(1) ) \dif \theta \\
        \iff \E \left[ \int_0^{\theta} (D^-)^3 e^{-\theta' D^-} \dif \theta' \right]
        &= \E \left[ (D^-)^3 \right] \theta + \littleo(\theta) \\
        \iff \E \left[ (D^-)^2 \right] - \E \left[ (D^-)^2 e^{-\theta D^-} \right]
        &= \E \left[ (D^-)^3 \right] \theta + \littleo(\theta) \\
        \iff \E \left[ (D^-)^2 e^{-\theta D^-} \right]
        &= \E \left[ (D^-)^2 \right] - \E \left[ (D^-)^3 \right] \theta + \littleo(\theta).
        \label{eq:asym20}
    \end{align}
    Repeating this method yields
    \begin{align}
        \E \left[ D^- e^{-\theta D^-} \right]
        &= \mu - \E \left[ (D^-)^2 \right] \theta + \tfrac{1}{2} \E \left[ (D^-)^3 \right] \theta^2 + \littleo(\theta^2),
        \label{eq:asym10}\\
        \text{and} \quad \E \left[ e^{-\theta D^-} \right]
        &= 1 - \mu \theta + \tfrac{1}{2} \E \left[ (D^-)^2 \right] \theta^2 - \tfrac{1}{6} \E \left[ (D^-)^3 \right] \theta^3 + \littleo(\theta^3).
        \label{eq:asym00}
    \end{align}
    Similary integrating the equation
    \begin{equation}
        \E \left[ (D^-)^2 D^+ \exp(-\theta D^-) \right] = \E \left[ (D^-)^2 D^+ \right] + \littleo(1)
    \end{equation}
    gives
    \begin{align}
        \E \left[ D^- D^+ e^{-\theta D^-} \right]
        &= \E \left[ D^- D^+ \right] - \E \left[ (D^-)^2 D^+ \right] \theta + \littleo(\theta),
        \label{eq:asym11}\\
        \text{and} \quad \E \left[ D^+ e^{-\theta D^-} \right]
        &= \mu \theta - \E \left[ D^- D^+ \right] \theta + \tfrac{1}{2} \E \left[ (D^-)^2 D^+ \right] \theta^2 + \littleo(\theta^2).
        \label{eq:asym01}
    \end{align}
    Integrating the equation
    \begin{equation}
        \E \left[ D^- (D^+)^2 \exp(-\theta D^-) \right] = \E \left[ D^- (D^+)^2 \right] + \littleo(1)
    \end{equation}
    gives
    \begin{equation}
        \E \left[ (D^+)^2 e^{-\theta D^-} \right] = \E \left[ (D^+)^2 \right] - \E \left[ D^- (D^+)^2 \right] \theta + \littleo(\theta). \label{eq:asym02}
    \end{equation}
    \cref{eq:asym00} gives the expansion of the normalising constant of the measure change. Taking the logarithm on both sides of \cref{eq:asym00} and expanding the right hand side gives the expansion for the cumulant generating function. Combining this with \cref{eq:asym10} and \cref{eq:asym01} gives the expansions for $\E_{\theta}[D^-]$ and $\E_{\theta}[D^+]$ respectively. Similarly \cref{eq:asym20}, \cref{eq:asym02} and \cref{eq:asym11} are used to get the exanpsions for $\var_{\theta}(D^-)$, $\var_{\theta}(D^+)$ and $\cov_{\theta}(D^-, D^+)$ respectively.
\end{proof}

\subsection{Local limit theorems}

\begin{lemma}
    Let $c$ be given by 
    \begin{equation}
        c \defeq \frac{\cov(D^-, D^- - D^+)}{\var(D^- - D^+)}
    \end{equation}
    such that $\cov(D^- - c(D^- - D^+), D^- - D^+) = 0$. Also let
    \begin{equation}
        \Theta_{n-m} \defeq \Xi^-_{n-m} - c\Delta_{n-m}.
    \end{equation}
    Then
    \begin{align}
        &\mathbb P\left( \Theta_{n-m} - \E[\Theta_{n-m}] = x, \Delta_{n-m} = y \right) \nonumber \\
        = &\frac{1}{\sqrt{2 \pi \sigma_1^2 n}} \exp\left(-\frac{1}{2 \sigma_1^2} \frac{x^2}{n}\right)
        \frac{1}{\sqrt{2 \pi \sigma_2^2 n}} \exp\left(-\frac{1}{2 \sigma_2^2} \frac{y^2}{n}\right) + \littleo\left( n^{-1} \right)
    \end{align}
    as $n \to \infty$ where the $\littleo(n^{-1})$ term is uniform in $x$ and $y$ and
    \begin{align}
        \sigma_1^2 = \var(D^- - c(D^- - D^+)) \quad \text{and} \quad \sigma_2^2 \var(D^- - D^+).
    \end{align}
    In addition
    \begin{equation}
        \mathbb P \left( \Delta_{n-m} = y \right)
        = \frac{1}{\sqrt{2 \pi \sigma_2^2 n}} \exp\left(-\frac{1}{2 \sigma_2^2} \frac{y^2}{n}\right) + \littleo\left( n^{-1/2} \right)
    \end{equation}
    as $n \to \infty$ where the $\littleo(n^{-1/2})$ term is uniform in $y$.
\end{lemma}

\begin{lemma}
    \begin{align}
        &\mathbb P_n\left( \Theta_{n-m} - \E_n[\Theta_{n-m}] = x, \Delta_{n-m} - \E_n[\Delta_{n-m}]= y \right) \nonumber \\
        = &\frac{1}{\sqrt{2 \pi \sigma_1^2 n}} \exp\left(-\frac{1}{2 \sigma_1^2} \frac{x^2}{n}\right)
        \frac{1}{\sqrt{2 \pi \sigma_2^2 n}} \exp\left(-\frac{1}{2 \sigma_2^2} \frac{y^2}{n}\right) + \littleo\left( n^{-1} \right)
    \end{align}
    as $n \to \infty$ where the $\littleo(n^{-1})$ term is uniform in $x$ and $y$.
\end{lemma}

\begin{lemma}
    \label{lem:mod-dev-local}
    \begin{equation}
        \mathbb P_n \left( \abs{\Omega_n} \geq n^{\frac{1}{2} + \epsilon},\ \Delta_{n-m} = \sum_{i=1}^m (k_i^+ - k_i^-) \right)
        \geq \mathbb P \left( \Delta_{n-m} = 0 \right) (1 + \littleo(1))
    \end{equation}
\end{lemma}

\subsection{Combining the parts}

\begin{proof}
    Firstly
    \begin{equation}
        \prod_{i=1}^m \frac{(n-i+1)\mu}{\sum_{k=i}^m k^-_i + \Xi^-_{n-m}} = \exp(X_n - Y_n)
    \end{equation}
    where
    \begin{equation}
        X_n = \sum_{i=1}^m \log\left( 1 - \frac{i-1}{n} \right)
        \quad \text{and} \quad
        Y_n = \sum_{i=1}^m \log\left( \frac{\sum_{k=i}^m k_i^- + \Xi^-_{n-m}}{\mu n} \right).
    \end{equation}
    Note
    \begin{equation}
        \sum_{j=i}^m k^-_j = s^-(m) - s^-(i - 1) + (m - i + 1) \nu_-
    \end{equation}
    Thus
    \begin{align}
        Y_n 
        &= \sum_{i=1}^m \log \left( \frac{s^-(m) - s^-(i-1) + (m - i + 1) \nu_- + \Omega^-_n + \mu n - \nu_- m}{\mu n} \right) \\
        &= \sum_{i=1}^m \log \left( 1 + A_n^i + B_n + C_n^i \right)
    \end{align}
    where
    \begin{align}
        A_n^i = - \frac{1}{\mu} \frac{1}{n} \left[ s^-(i-1) - s^-(m) \right], \quad
        B_n = \frac{1}{\mu} \frac{1}{n} \Omega^-_n, \quad
        C_n^i = - \frac{\nu_-}{\mu} \frac{1}{n} (i-1).
    \end{align}
    When expanding $\log(1 + A_n^i + B_n + C_n^i)$, the summation contributes order $m = O(n^{2/3})$. Thus we keep terms of order $\Omega(n^{-2/3})$ in the expansion. Write
    \begin{equation}
        \omegaevent_n \defeq \left\{ \abs{\Omega^-_n} \leq n^{\frac{1}{2} + \epsilon} \right\}
    \end{equation}
    On the event $\omegaevent_n$, we can check that the $A_n, B_n, C_n^i$ and $(C_n^i)^2$ terms are the only terms in the expansion which has order $\Omega(n^{-2/3})$, moreover
    \begin{equation}
        \sum_{i=1}^m C_n^i = - \frac{\nu_-}{2 \mu} \frac{m^2}{n} + \littleo(1) \quad \text{and} \quad
        \sum_{i=1}^m (C_n^i)^2 = \frac{\nu_-^2}{3 \mu^2} \frac{m^3}{n^2} + \littleo(1).
    \end{equation}
    Therefore
    \begin{align}
        Y_n
        &= \sum_{i=1}^m (A_n^i + B_n + C_n^i - \tfrac{1}{2} (C_n^i)^2) + \littleo(1) \\
        &= - \frac{1}{\mu} \frac{1}{n} \sum_{i=0}^m \left( s^-(i) - s^-(m) \right)
        + \frac{1}{\mu} \frac{m}{n} \Omega_n^-
        - \frac{\nu_-}{2\mu} \frac{m^2}{n} - \frac{\nu_-^2}{6 \mu^2} \frac{m^3}{n^2} + \littleo(1).
    \end{align}
    where we use that $\sum_{i=1}^m \left( s^-(i-1) - s^-(m) \right) = \sum_{i=0}^m \left( s^-(i) - s^-(m) \right)$.

    Similarly we can expand $X_n$ as
    \begin{equation}
        X_n = - \frac{1}{2} \frac{m}{n} - \frac{1}{3} \frac{m^3}{n^2} + \littleo(1).
    \end{equation}

    Thus
    \begin{align}
        \prod_{i=1}^m \frac{(n-i+1)\mu}{\sum_{k=i}^m k^-_i + \Xi^-_{n-m}}
        &\geq \exp \Bigg( \frac{1}{\mu} \frac{1}{n} \sum_{i=1}^m (s^-(i) - s^-(m)) \nonumber \\
        &\hspace{4em} - \frac{1}{\mu} \frac{m}{n} \Omega^-_n + \frac{\nu_- - \mu}{2 \mu} \frac{m^2}{n} + \frac{\nu_-^2 - \mu^2}{6 \mu^2} \frac{m^3}{n^2} \Bigg) \indi_{\omegaevent_n}
    \end{align}
    In addition measure change can be expanded as
    \begin{equation}
        \diff{\Pr_n}{\Pr} = \exp \left( 
            - \frac{1}{\mu} \frac{m}{n} \Omega_n^- + \frac{\nu_- - \mu}{2\mu} \frac{m^2}{n}
            + \frac{\nu_-^2 - \mu^2}{6 \mu^2} \frac{m^3}{n^2} + \frac{\biasvar}{6 \mu^2} \frac{m^3}{n^2} + \littleo(1)
         \right).
    \end{equation}
    Hence
    \begin{align}
        &\conmc^n_m(\vect{k}_1, \ldots, \vect{k}_m) \\
        =& \frac{1}{\Pr(\Delta_n = 0)} \E \left[\prod_{i=1}^m \frac{(n-i+1)\mu}{\sum_{k=i}^m k^-_i + \Xi^-_{n-m}} \indi_{A_n} \right] \\
        \geq& \frac{1}{\Pr(\Delta_n = 0)} \E_n \left[ \exp \left(
                \frac{1}{\mu n} \sum_{i=1}^m \left( s^-(i) - s^-(m) \right)
                - \frac{\biasvar}{6 \mu^2} \frac{m^3}{n^2} + \littleo(1)
            \right) \indi_{\omegaevent_n \cap A_n} \right] \\
        \geq& \exp \left(
                \frac{1}{\mu n} \sum_{i=1}^m \left( s^-(i) - s^-(m) \right)
                - \frac{\biasvar}{6 \mu^2} \frac{m^3}{n^2}
            \right) (1 + \littleo(1)) \frac{\Pr_n(\omegaevent\cap A_n)}{\Pr(\Delta_n = 0)}.
    \end{align}

    Finally
    \begin{equation}
        \frac{\Pr_n(\omegaevent\cap A_n)}{\Pr(\Delta_n = 0)} \geq 1 + \littleo(1)
    \end{equation}
    as $n \to \infty$ by \cref{lem:mod-dev-local} which gives the desired final result.
\end{proof}

\printbibliography

\end{document}