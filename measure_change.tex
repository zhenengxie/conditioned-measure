\documentclass[draft]{scrartcl}

\usepackage{mathtools, amsfonts, dsfont}
\usepackage{diffcoeff}

\usepackage[backend=biber]{biblatex}

\addbibresource{references.bib}

\usepackage{varioref}
\usepackage{hyperref}
\usepackage{cleveref}

\usepackage{thmtools}

\declaretheorem[thmbox=M]{theorem}
\declaretheorem[thmbox=M]{lemma}

\usepackage{amsthm}

\newcommand{\R}{\mathbb R}
\newcommand{\N}{\mathbb N}
\newcommand{\E}{\mathbb E}
\newcommand{\littleo}{o}
\newcommand{\bigo}{O}
\newcommand{\biasD}{\hat{D}}
\newcommand{\unconmc}{\psi}
\newcommand{\conmc}{\phi}
\newcommand{\defeq}{:=}
\newcommand{\eqdist}{\overset{d}{=}}

\newcommand{\indi}{\mathds{1}}

\newcommand{\vect}{\mathbf}
\renewcommand{\Pr}{\mathbb P}
\newcommand{\Qr}{\mathbb Q}

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}

\newcommand{\biasvar}{\var(Z^-)}
\newcommand{\diffsd}{\tilde{\sigma}}

\title{Measure change under conditioning}
\author{}
\date{}

\begin{document}

\maketitle

\section{Setup and notation}

\begin{itemize}
    \item Let $\vect{D} = (D^-, D^+)$ a $\N \times \N$-valued random variable and let 
        \begin{align}
            \mu &\defeq \E[D^-] = \E[D^+] \\
            \beta_- &\defeq \E[(D^-)^2] \\
            \gamma_- &\defeq \E[(D^-)^3] \\
            \rho &\defeq \E[D^- D^+]
        \end{align}
    \item Let $\vect{D}_1, \ldots, \vect{D}_n$ be i.i.d.\ copies of $\vect{D}$ let 
        \begin{align}
            \Xi^{\pm}_{n-m} &\defeq \sum_{i = m + 1}^n D^{\pm}_i \\
            \Delta_{n-m} &\defeq \Xi^-_{n-m} - \Xi^+_{n-m}.
        \end{align}
    \item Let $\Sigma_n$ be a sized biased ordering of $\vect{D}_1, \ldots, \vect{D}_n$ be in-degree. Rigirously $\Sigma_n$ is a random permutation on $\{1, \ldots, n\}$ such that
        \begin{equation}
            \Pr(\Sigma_n = \sigma \mid D^-_1 = k^-_1, \ldots, D^-_n = k^-_n) \defeq
            \prod_{i=1}^n \frac{k^-_{\sigma(i)}}{\sum_{j=i}^n k^-_{\sigma(j)}}.
            %\frac{k^-_{\sigma(1)}}{\sum_{i=1}^n k^-_{\sigma(i)}} \frac{k^-_{\sigma(2)}}{\sum_{i=2}^n k^-_{\sigma(i)}} \cdots
            %\frac{k^-_{\sigma(n)}}{k^-_{\sigma(n)}}.
        \end{equation}
    \item Let $(\vect{\biasD}_{n, 1}, \ldots, \vect{\biasD}_{n, n}) \defeq (\vect{D}_{\Sigma_n(1)}, \ldots, \vect{D}_{\Sigma_n(n)})$.
    \item Let $\vect{Z}_1, \vect{Z}_2, \ldots$ be i.i.d.\ with law
        \begin{equation}
            \Pr(Z^-_i = k^-, Z^+_i = k^+) \defeq \frac{1}{\mu} k^- \Pr(D^- = k^-, D^+ = k^+).
        \end{equation}
        Then
        \begin{align}
            \nu_- &\defeq \E[Z^-_i] = \tfrac{\beta_-}{\mu} \\
            \nu_+ &\defeq \E[Z^+_i] = \tfrac{\rho}{\mu}
        \end{align}
\end{itemize}

\section{Exact form of the measure change}

First we state the form of the measure change between $\vect{\biasD}_{n, 1}, \ldots, \vect{\biasD}_{n, m}$ and $\vect{Z}_1, \ldots, \vect{Z}_m$. The proof of \cite[Proposition 4.2]{conchon--kerjan_stable_2020} covers the undirected configuration model but the same proof works for the directed case.

\begin{theorem}
    \label{thm:unconditioned-mc}
    For any test function $u: (\N \times \N)^m \to \R$, 
    \begin{equation}
        \E[u(\vect{\biasD}_{n, 1}, \ldots, \vect{\biasD}_{n, m})] =
        \E[u(\vect{Z}_1, \ldots, \vect{Z}_m) \unconmc^n_m(\vect{Z}_1, \ldots, \vect{Z}_m)]
    \end{equation}
    where
    \begin{equation}
        \unconmc^n_m(\vect{k}_1, \ldots, \vect{k}_m) \defeq
        \E \left[ \prod_{i=1}^m \frac{(n - i + 1)\mu}{\sum_{j=i}^n k_j^- + \Xi^-_{n - m}}\right].
    \end{equation}
\end{theorem}

Now we gives the form of the measure change after conditioning on the event
\begin{equation}
    \{\Delta_n = 0\}
    = \left\{ \sum_{i=1}^n \biasD^-_{n, i} = \sum_{i=1}^n \biasD^+_{n, i} \right\}
    = \left\{ \sum_{i=1}^n D^-_i = \sum_{i=1}^n D^+_i \right\}.
\end{equation}

\begin{theorem}
    For any test function $u: (\N \times \N)^m \to \R$, 
    \begin{equation}
        \E\left[u(\vect{\biasD}_{n, 1}, \ldots, \vect{\biasD}_{n, m}) \mid \Delta_n = 0 \right] =
        \E[u(\vect{Z}_1, \ldots, \vect{Z}_m) \conmc^n_m(\vect{Z}_1, \ldots, \vect{Z}_m)]
        \label{eq:conditioned-measure-change-condition}
    \end{equation}
    where
    \begin{equation}
        \conmc^n_m(\vect{k}_1, \ldots, \vect{k}_m) = \frac{
        \E \left[ \prod_{i=1}^m \frac{(n - i + 1)\mu}{\sum_{j=i}^n k_j^- + \Xi^-_{n - m}}
        \indi \left\{ \Delta_{n-m} = \sum_{i=1}^m (k^+_i - k^-_i) \right\}\right]
        }{\Pr(\Delta_n = 0)}.
    \end{equation}
\end{theorem}

\begin{proof}
    By \cref{thm:unconditioned-mc}, since
    \begin{equation}
        \unconmc^n_n(\vect{k}_1, \ldots, \vect{k}_m) =
        \prod_{i=1}^n \frac{(n - i + 1) \mu}{\sum_{j=i}^n k_j^-}
    \end{equation}
    we have
    \begin{align}
        &\E\left[u(\vect{\biasD}_{n, 1}, \ldots, \vect{\biasD}_{n, m}) \indi\left\{ \sum_{i=1}^n (\biasD^-_{n, i} - \biasD^+_{n, i}) = 0 \right\} \right] \\
        = &\E\left[u(\vect{Z}_1, \ldots, \vect{Z}_m) \indi\left\{ \sum_{i=1}^n (Z_i^- - Z_i^+) = 0 \right\} \prod_{i=1}^n \frac{(n - i + 1) \mu}{\sum_{j=i}^n Z_j^-} \right] \\
        = &\E\Bigg[u(\vect{Z}_1, \ldots, \vect{Z}_m) \E \Bigg[ \indi\left\{ \sum_{i=m+1}^n (Z_i^- - Z_i^+) = \sum_{i=1}^m (Z^+_i - Z^-_i) \right\} \nonumber \\
       & \hspace{5.5em} \times \prod_{i=1}^m \frac{(n - i + 1)\mu}{\sum_{j=i}^m Z_j^- + \sum_{j = m+1}^n Z_j^-} \prod_{i=m+1}^n \frac{(n - i + 1) \mu}{\sum_{j=i}^n Z_j^-} \mid \vect{Z}_1, \ldots, \vect{Z}_m \Bigg] \Bigg].
    \end{align}
    Thus \cref{eq:conditioned-measure-change-condition} holds with
    \begin{align}
        \conmc^n_m(\vect{k}_1, \ldots, \vect{k}_m)
        &= \frac{1}{\Pr(\Delta_n = 0)}\E\Bigg[ \indi\left\{ \sum_{i=m+1}^n (Z_i^- - Z_i^+) = \sum_{i=1}^m (k^+_i - k^-_i)\right\} \nonumber \\
        &\hspace{6em} \times \prod_{i=1}^m \frac{(n - i + 1)\mu}{\sum_{j=i}^m k_j^- + \sum_{j = m+1}^n Z_j^-} \prod_{i=m+1}^n \frac{(n - i + 1) \mu}{\sum_{j=i}^n Z_j^-} \Bigg].
    \end{align}
    Since the $Z_i$ are i.i.d.\ we can shift indices to get
    \begin{align}
        \conmc^n_m(\vect{k}_1, \ldots, \vect{k}_m)
        &= \frac{1}{\Pr(\Delta_n = 0)}\E\Bigg[ \indi\left\{ \sum_{i=1}^{n-m} (Z_i^- - Z_i^+) = \sum_{i=1}^m (k^+_i - k^-_i)\right\} \nonumber \\
        &\hspace{5.5em} \times \prod_{i=1}^m \frac{(n - i + 1)\mu}{\sum_{j=i}^m k_j^- + \sum_{j = 1}^{n -m} Z_j^-} \prod_{i=1}^{n-m} \frac{(n - m - i + 1) \mu}{\sum_{j=i}^{n - m} Z_j^-} \Bigg].
    \end{align}
    Note that
    \begin{equation}
        \prod_{i=1}^{n-m} \frac{(n - m - i + 1) \mu}{\sum_{j=i}^{n - m} Z_j^-} = \unconmc^{n-m}_{n-m}(\vect{Z}_1, \ldots, \vect{Z}_{n-m})
    \end{equation}
    therefore by \cref{thm:unconditioned-mc}
    \begin{align}
        \conmc^n_m(\vect{k}_1, \ldots, \vect{k}_m)
        &= \frac{1}{\Pr(\Delta_n = 0)}\E\Bigg[ \indi\left\{ \sum_{i=1}^{n-m} (\biasD_{n-m, i}^- - \biasD_{n-m, i}^+) = \sum_{i=1}^m (k^+_i - k^-_i)\right\} \nonumber \\
        &\hspace{9em} \times \prod_{i=1}^m \frac{(n - i + 1)\mu}{\sum_{j=i}^m k_j^- + \sum_{j = 1}^{n -m} \biasD_{n-m, j}^-} \Bigg].
    \end{align}
    Finally since
    \begin{equation}
        \sum_{i=1}^{n-m} \biasD^{\pm}_{n-m, i} = \sum_{i=1}^{n-m} D^{\pm}_i \eqdist \sum_{i=m+1}^n D^{\pm}_i = \Xi_{n-m}^{\pm}
    \end{equation}
    we obtain the desired result of
    \begin{equation}
        \conmc^n_m(\vect{k}_1, \ldots, \vect{k}_m) = \frac{
        \E \left[ \prod_{i=1}^m \frac{(n - i + 1)\mu}{\sum_{j=i}^n k_j^- + \Xi^-_{n - m}}
        \indi \left\{ \Delta_{n-m} = \sum_{i=1}^m (k^+_i - k^-_i) \right\}\right]
        }{\Pr(\Delta_n = 0)}.
    \end{equation}
\end{proof}

\section{Approximation of the measure change}

We explore the scaling of the measure change $\conmc^n_m$ as $n \to \infty$ in the regime $m(n) = \Theta(n^{2/3})$.

\begin{theorem}
    Define
    \begin{equation}
        s^{\pm}(i) \defeq \textstyle{\sum_{j=1}^i (k_i^{\pm} - \nu_{\pm})}.
    \end{equation}
    Assume that $m = \Theta(n^{2/3})$ and
    \begin{equation}
        \label{eq:s-condition}
        \abs{s^{\pm}(i)} \leq n^{\frac{1}{3} + \epsilon} \quad \forall i = 1, \ldots, m.
    \end{equation}
    Then
    \begin{equation}
        \conmc^n_m(\vect{k}_1, \ldots, \vect{k}_m)
        \geq \exp\left( \frac{1}{\mu n} \sum_{i=0}^m (s^-(i) - s^-(m)) - \frac{\var(Z^-)}{6 \mu^2} \frac{m^3}{n^2} \right) + \littleo(1)
    \end{equation}
    where the $\littleo(1)$ term is independent of $\vect{k}_1, \ldots, \vect{k}_m$ satisfying our assumptions.
\end{theorem}

This result only provides a lower bound, but this proves to be sufficient since we can show asymptotically that the lower bound has expectation 1. This shows we haven't lost a significant amount of mass in the limit. The condition on $\vect{k}_1, \ldots, \vect{k}_m$ occurs with high probability when $\vect{k}_i = \vect{Z}_i$ since the $s^-$ and $s^+$ become centered random walks.

\subsection{Exponential tilting}

The key ingredient of the proof is to tilt the measure exponentially so that $\Delta_{n-m}$ has mean approximately $\sum_{i=1}^m (k^+_i - k^-_i)$.

\begin{lemma}
    Define an measure $\Pr_{\theta}$, for $\theta \geq 0$, by its Radon--Nikodym derivative
    \begin{equation}
        \diff{\Pr_{\theta}}{\Pr} \defeq \exp\left( - \Xi^-_{n-m} - (n-m) \alpha(\theta) \right)
        \quad \text{where} \quad
        \alpha(\theta) \defeq \log \E \left[ e^{-\theta D^-} \right].
    \end{equation}
    Take $\theta_n = \frac{1}{\mu} \frac{m}{n}$. Then
    \begin{align}
        (n-m)\alpha(\theta_n) &= -m + \frac{\nu_- - \mu}{2 \mu} \frac{m^2}{n} - \frac{\biasvar + \nu_-^2 - \mu^2}{6 \mu^2} \frac{m^3}{n^2} + \littleo(1), \\
        \E_{\theta_n}[D^{\pm}] &= mu - (\nu_{\pm} - \mu) \frac{m}{n} + \bigo(n^{-2/3}), \\
        \var_{\theta_n}(D^{\pm}) &= \var(D^{\pm}) + \littleo(1), \\
        \text{and}\ \cov_{\theta_n}(D^-, D^+) &= \cov(D^-, D^+) + \littleo(1).
    \end{align}
\end{lemma}

\subsection{Local limit theorems}

Next we recall a local limit theorem for triangular arrays on lattices. For this section it is convenient to view all the variables as living in the same probabilty space rather than applying a measure change.

\begin{lemma}
    Writing $\diffsd^2 = \var(D^- - D^+)$, 
    \begin{equation}
        \Pr_n\left( \Delta_{n-m} = \sum_{i=1}^m (k^+_i - k^-_i) \right) \sim \frac{1}{\sqrt{2 \pi \diffsd^2 n}}.
    \end{equation}
\end{lemma}

When expanding the measure change, instead of expanding in terms of $\Xi^-_{n-m}$ it is convenient to expand in terms of $\Pr_n$-centering of $\Xi^-_{n-m}$, written $\Omega^-_n$. The following lemma establishes a probabilistic bound on the variance of $\Omega^-_n$ under the conditioning.

\begin{lemma}
    Define
    \begin{equation}
        \Omega_n^- \defeq \Xi^-_{n-m} - \E_n\left[ \Xi^-_{n-m} \right].
    \end{equation}
    Then
    \begin{equation}
        \lim_{n \to \infty} \Pr_n\left( \abs{\Omega^-_{n-m}} \leq n^{\frac{1}{2} + \epsilon} \Biggm| \Delta_{n-m} = \sum_{i=1}^m (k^+_i - k^-_i) \right) = 1.
    \end{equation}
\end{lemma}

\subsection{Combining the parts}

\begin{proof}
    Firstly
    \begin{equation}
        \prod_{i=1}^m \frac{(n-i+1)\mu}{\sum_{k=i}^m k^-_i + \Xi^-_{n-m}} = \exp(X_n - Y_n)
    \end{equation}
    where
    \begin{equation}
        X_n = \sum_{i=1}^m \log\left( 1 - \frac{i-1}{n} \right)
        \quad \text{and} \quad
        Y_n = \sum_{i=1}^m \log\left( \frac{\sum_{k=i}^m k_i^- + \Xi^-_{n-m}}{\mu n} \right).
    \end{equation}
    We can expand $X_n$ as
    \begin{equation}
        X_n = - \frac{1}{2} \frac{m}{n} - \frac{1}{3} \frac{m^3}{n^2} + \littleo(1).
    \end{equation}
    Note
    \begin{equation}
        \sum_{j=i}^m k^-_j = s^-(m) - s^-(i - 1) + (m - i + 1) \mu
    \end{equation}
    and by REFERENCE
    \begin{equation}
        \Xi^-_{n-m} = \Omega^-_n + (n - m) \E_n[D^-] = \mu n - \nu_- m + r_n
    \end{equation}
    where $r_n = O(n^{1/3})$. Thus
    \begin{equation}
        Y_n = 
    \end{equation}
\end{proof}

\printbibliography

\end{document}